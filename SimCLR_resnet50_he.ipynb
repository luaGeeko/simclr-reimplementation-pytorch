{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7nfrxp3A1C1"
      },
      "source": [
        "# with LN it learns but is very slow, lets complete with BN and we will look into LN\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q58ZMoKDBGe5"
      },
      "source": [
        "import numpy as np\n",
        "import warnings\n",
        "from imgaug import augmenters as ia\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.models import Model\n",
        "from keras.preprocessing import image\n",
        "import keras.backend as K\n",
        "from keras.utils import layer_utils\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.engine.topology import get_source_inputs\n",
        "\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from datetime import datetime\n",
        "from tensorflow.python.keras.utils.data_utils import Sequence\n",
        "import random\n",
        "!pip install --upgrade imgaug"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4RjcReFBNiV"
      },
      "source": [
        "print(\"TensorFlow version: {}\".format(tf.__version__))\n",
        "print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n",
        "import imgaug\n",
        "print (imgaug.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sk1NkL4hBUIo"
      },
      "source": [
        "### class Augmentations:\n",
        "\n",
        "class Augmentations:\n",
        "    \"\"\" three data augmentation operators as mentioned to paper \"\"\"\n",
        "    def __init__(self, p = [0.4, 0.4, 0.2], severity=2):\n",
        "        self.severity = severity\n",
        "        self.p = p\n",
        "\n",
        "    def get_opertors(self):\n",
        "        operators = [self.random_crop, self.color_distortion, self.gauss_blur]\n",
        "        ops = random.choices(operators, k=2, weights=self.p)\n",
        "        return ops\n",
        "\n",
        "    def random_crop(self, image):\n",
        "        seq = ia.Sequential([\n",
        "                ia.Crop(px=(0, 10)),\n",
        "                ia.Sometimes(0.8, ia.Fliplr(0.5)),\n",
        "                ia.Sometimes(0.8, ia.Flipud(0.5)),\n",
        "                ia.Resize({'height': 32, 'width': 32})])\n",
        "\n",
        "        images_aug = seq(images=image)\n",
        "        return images_aug / 255.\n",
        "\n",
        "    #ia.imgcorruptlike.Contrast(severity=self.severity)\n",
        "    def color_distortion(self, image):\n",
        "        image = image.astype(np.uint8)\n",
        "        seq = ia.Sequential([ia.MultiplyBrightness((0.5, 1.5)),\n",
        "                             ia.LinearContrast((0.4, 1.6)),\n",
        "                             ia.MultiplySaturation((0.5, 1.5)),\n",
        "                             ia.Sometimes(0.2, ia.Grayscale(alpha=(0.0, 1.0))),\n",
        "                             ia.Resize({'height': 32, 'width': 32})])\n",
        "\n",
        "\n",
        "        images_aug = seq(images=image)\n",
        "        images_aug.astype(np.float32)\n",
        "        return images_aug / 255.\n",
        "\n",
        "    def gauss_blur(self, image):\n",
        "        seq = ia.Sequential([ia.GaussianBlur(sigma=(0, 3.0)),\n",
        "                            ia.Resize({'height': 32, 'width': 32})])\n",
        "        #gb = ia.GaussianBlur(sigma=(0, 3.0))\n",
        "        aug = seq(images=image)\n",
        "        return aug / 255.\n",
        "\n",
        "    def apply_operators(self, image):\n",
        "        ops = self.get_opertors()\n",
        "        #print (\"### operators applied\", ops)\n",
        "        first_op = ops[0](image)\n",
        "        second_op = ops[1](image)\n",
        "        stack = np.stack((first_op, second_op), axis=1)\n",
        "        reshape_stack = stack.reshape(stack.shape[0] * stack.shape[1], stack.shape[2], stack.shape[3], stack.shape[4])\n",
        "        return reshape_stack\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boZFUS-3BaH0"
      },
      "source": [
        "#### DATA GENERATOR PIPELINE\n",
        "\n",
        "class CIFAR10Sequence(Sequence):\n",
        "    def __init__(self, x_set, batch_size, shuffle=True, aug=False):\n",
        "        self.x  = x_set\n",
        "        self.batch_size = batch_size\n",
        "        self.aug = aug\n",
        "        self.indices = np.arange(self.x.shape[0])\n",
        "        np.random.shuffle(self.indices)\n",
        "        self.shuffle = shuffle\n",
        "        self.augment = Augmentations()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inds = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        #batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        batch_x = self.x[inds]\n",
        "        #batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "        #batch_x = np.array(batch_x)\n",
        "        #batch_y = np.array(batch_y)\n",
        "\n",
        "        if self.aug:\n",
        "            #print (\"Applying augmentation\")\n",
        "            #auuu = self.augment.apply_operators(np.array(batch_x))\n",
        "            #print (\"aug total for the model\", auuu.shape)\n",
        "            return self.augment.apply_operators(np.array(batch_x))\n",
        "        else:\n",
        "            #return batch_x, batch_y\n",
        "            return np.array(batch_x)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        np.random.shuffle(self.indices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLRe4uO0pPPi"
      },
      "source": [
        "# with 'he_normal'\n",
        "def identity_block(input_tensor, kernel_size, filters, stage, block):\n",
        "    \"\"\"The identity block is the block that has no conv layer at shortcut.\n",
        "    # Arguments\n",
        "        input_tensor: input tensor\n",
        "        kernel_size: default 3, the kernel size of\n",
        "            middle conv layer at main path\n",
        "        filters: list of integers, the filters of 3 conv layer at main path\n",
        "        stage: integer, current stage label, used for generating layer names\n",
        "        block: 'a','b'..., current block label, used for generating layer names\n",
        "    # Returns\n",
        "        Output tensor for the block.\n",
        "    \"\"\"\n",
        "    filters1, filters2, filters3 = filters\n",
        "    if K.image_data_format() == 'channels_last':\n",
        "        bn_axis = 3\n",
        "    else:\n",
        "        bn_axis = 1\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "\n",
        "    x = layers.Conv2D(filters1, (1, 1),\n",
        "                      kernel_initializer='he_normal', kernel_regularizer=l2(l=1e-4),\n",
        "                      name=conv_name_base + '2a')(input_tensor)\n",
        "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    x = layers.Conv2D(filters2, kernel_size,\n",
        "                      padding='same',\n",
        "                      kernel_initializer='he_normal', kernel_regularizer=l2(l=1e-4),\n",
        "                      name=conv_name_base + '2b')(x)\n",
        "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    x = layers.Conv2D(filters3, (1, 1),\n",
        "                      kernel_initializer='he_normal', kernel_regularizer=l2(l=1e-4),\n",
        "                      name=conv_name_base + '2c')(x)\n",
        "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n",
        "\n",
        "    x = layers.add([x, input_tensor])\n",
        "    x = layers.Activation('relu')(x)\n",
        "    return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8fwkDpxpWWC"
      },
      "source": [
        "def conv_block(input_tensor,\n",
        "               kernel_size,\n",
        "               filters,\n",
        "               stage,\n",
        "               block,\n",
        "               strides=(2, 2)):\n",
        "    \"\"\"A block that has a conv layer at shortcut.\n",
        "    # Arguments\n",
        "        input_tensor: input tensor\n",
        "        kernel_size: default 3, the kernel size of\n",
        "            middle conv layer at main path\n",
        "        filters: list of integers, the filters of 3 conv layer at main path\n",
        "        stage: integer, current stage label, used for generating layer names\n",
        "        block: 'a','b'..., current block label, used for generating layer names\n",
        "        strides: Strides for the first conv layer in the block.\n",
        "    # Returns\n",
        "        Output tensor for the block.\n",
        "    Note that from stage 3,\n",
        "    the first conv layer at main path is with strides=(2, 2)\n",
        "    And the shortcut should have strides=(2, 2) as well\n",
        "    \"\"\"\n",
        "    filters1, filters2, filters3 = filters\n",
        "    if K.image_data_format() == 'channels_last':\n",
        "        bn_axis = 3\n",
        "    else:\n",
        "        bn_axis = 1\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "\n",
        "    x = layers.Conv2D(filters1, (1, 1), strides=strides,\n",
        "                      kernel_initializer='he_normal', kernel_regularizer=l2(l=1e-4),\n",
        "                      name=conv_name_base + '2a')(input_tensor)\n",
        "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    x = layers.Conv2D(filters2, kernel_size, padding='same',\n",
        "                      kernel_initializer='he_normal', kernel_regularizer=l2(l=1e-4),\n",
        "                      name=conv_name_base + '2b')(x)\n",
        "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    x = layers.Conv2D(filters3, (1, 1),\n",
        "                      kernel_initializer='he_normal', kernel_regularizer=l2(l=1e-4),\n",
        "                      name=conv_name_base + '2c')(x)\n",
        "    x = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n",
        "\n",
        "    shortcut = layers.Conv2D(filters3, (1, 1), strides=strides,\n",
        "                             kernel_initializer='he_normal', kernel_regularizer=l2(l=1e-4),\n",
        "                             name=conv_name_base + '1')(input_tensor)\n",
        "    shortcut = layers.BatchNormalization(\n",
        "        axis=bn_axis, name=bn_name_base + '1')(shortcut)\n",
        "\n",
        "    x = layers.add([x, shortcut])\n",
        "    x = layers.Activation('relu')(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZs6QobYpYlz"
      },
      "source": [
        "def ResNet50(include_top=True, input_tensor=None, input_shape=(32, 32, 3), batch_size=10):\n",
        "\n",
        "    if input_tensor is None:\n",
        "        img_input = layers.Input(shape=input_shape, batch_size=batch_size)\n",
        "    else:\n",
        "        if not K.is_keras_tensor(input_tensor):\n",
        "            img_input = layers.Input(tensor=input_tensor, shape=input_shape)\n",
        "        else:\n",
        "            img_input = input_tensor\n",
        "    if K.image_data_format() == 'channels_last':\n",
        "        bn_axis = 3\n",
        "    else:\n",
        "        bn_axis = 1\n",
        "\n",
        "    x = layers.ZeroPadding2D((3, 3))(img_input)\n",
        "    x = layers.Conv2D(64, (3, 3), strides=(1, 1), kernel_initializer='he_normal', kernel_regularizer=l2(l=1e-4), name='conv1')(x)\n",
        "    x = layers.BatchNormalization(axis=bn_axis, name='bn_conv1')(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    #x = layers.MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
        "\n",
        "    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1))\n",
        "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b')\n",
        "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='c')\n",
        "\n",
        "    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a')\n",
        "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='b')\n",
        "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='c')\n",
        "    x = identity_block(x, 3, [128, 128, 512], stage=3, block='d')\n",
        "\n",
        "    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a')\n",
        "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='b')\n",
        "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='c')\n",
        "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='d')\n",
        "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='e')\n",
        "    x = identity_block(x, 3, [256, 256, 1024], stage=4, block='f')\n",
        "\n",
        "    x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a')\n",
        "    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b')\n",
        "    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c')\n",
        "\n",
        "    x = layers.GlobalAveragePooling2D(name='global_avg_pool')(x)\n",
        "\n",
        "    # non linear projection head\n",
        "    #x = layers.Flatten()(x)\n",
        "    #x = layers.BatchNormalization(axis=bn_axis, name='bn_flatten')(x)\n",
        "    #x = layers.Dense(2048, activation='relu', name='fc2048')(x)\n",
        "    #x = layers.BatchNormalization(axis=bn_axis, name='bn_fc2048')(x)\n",
        "    x = layers.Dense(1024, activation='relu', name='fc1024')(x)\n",
        "    x = layers.Dense(128, name='fc128')(x)\n",
        "\n",
        "    # Ensure that the model takes into account\n",
        "    # any potential predecessors of `input_tensor`.\n",
        "    if input_tensor is not None:\n",
        "        inputs = get_source_inputs(input_tensor)\n",
        "    else:\n",
        "        inputs = img_input\n",
        "    # Create model.\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=x, name='resnet50_he')\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4cehW3fBydv"
      },
      "source": [
        "from tensorflow.keras import backend as K\n",
        "\n",
        "## CUSTOM LOSS\n",
        "def test_custom_cosine_an(x, tau=0.5, epsilon=1e-8):\n",
        "    # get cosine similarity between between each pair\n",
        "    def cosine_score(x):\n",
        "        vec_norm = tf.norm(x, axis=1, keepdims=True)\n",
        "        magnitude = tf.multiply(vec_norm, tf.transpose(vec_norm))\n",
        "        magnitude_clipped = tf.clip_by_value(magnitude, clip_value_min=0.000001, clip_value_max=10000)\n",
        "        dot_product = tf.matmul(x, x, transpose_b=True)\n",
        "        # add tau value later\n",
        "        #scale_temp_magnitude = magnitude * tau\n",
        "        cosine_theta = dot_product / magnitude_clipped\n",
        "        return cosine_theta\n",
        "\n",
        "    def create_indices(x):\n",
        "        # incrementing ids\n",
        "        ids = K.shape(x)[0]\n",
        "        range_output = layers.Lambda(lambda x: tf.range(x))(ids)\n",
        "        incre_ids = layers.Lambda(lambda x: tf.range(0, x.shape[0], 2))(range_output)\n",
        "        reshape_incre_ids = layers.Lambda(lambda x: tf.reshape(x, [-1, 1]))(incre_ids)\n",
        "        increment_output = layers.Lambda(lambda x: tf.add(x[::2], 1))(range_output)\n",
        "        # scatter and update\n",
        "        updated_w_increment = tf.tensor_scatter_nd_update(range_output, reshape_incre_ids, increment_output)\n",
        "\n",
        "        # decrement ids\n",
        "        decre_ids = layers.Lambda(lambda x: tf.range(1, x.shape[0], 2))(range_output)\n",
        "        reshape_decre_ids = layers.Lambda(lambda x: tf.reshape(x, [-1, 1]))(decre_ids)\n",
        "        decrement_output = layers.Lambda(lambda x: tf.subtract(x[1::2], 1))(updated_w_increment)\n",
        "        updated_w_decrement = tf.tensor_scatter_nd_update(updated_w_increment, reshape_decre_ids, decrement_output)\n",
        "        return updated_w_decrement\n",
        "\n",
        "    cosine_theta_scores = cosine_score(x)\n",
        "    # temperature scaling  exp(score/ tau)\n",
        "    exp_cosine = tf.exp( cosine_theta_scores / tau)\n",
        "    # getting all positive pairs\n",
        "    ids = create_indices(exp_cosine)\n",
        "    exp_cosine_diag = tf.gather(exp_cosine, ids)\n",
        "    deno = tf.reduce_sum(exp_cosine_diag, axis=0) - tf.exp(1 / tau)\n",
        "    loss = tf.linalg.diag_part(exp_cosine_diag) / deno\n",
        "    log_loss = -tf.math.log(tf.reduce_mean(loss))\n",
        "    #print (\"log loss\", log_loss)\n",
        "    return log_loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ly4Vpvd4sWox"
      },
      "source": [
        "## CUSTOM LOSS\n",
        "def test_custom_cosine(x, tau=0.5, epsilon=1e-8):\n",
        "    # get cosine similarity between between each pair\n",
        "\n",
        "    def cosine_score(x):\n",
        "        vec_norm = tf.norm(x, axis=1, keepdims=True)\n",
        "        magnitude = tf.multiply(vec_norm, tf.transpose(vec_norm))\n",
        "        magnitude_clipped = tf.clip_by_value(magnitude, clip_value_min=0.000001, clip_value_max=10000)\n",
        "        dot_product = tf.matmul(x, x, transpose_b=True)\n",
        "        # add tau value later\n",
        "        #scale_temp_magnitude = magnitude * tau\n",
        "        cosine_theta = dot_product / magnitude_clipped\n",
        "        return cosine_theta\n",
        "\n",
        "    cosine_theta_scores = cosine_score(x)\n",
        "    # temperature scaling  exp(score/ tau)\n",
        "    exp_cosine = tf.exp( cosine_theta_scores / tau)\n",
        "    # getting all positive pairs\n",
        "    ids = np.arange(exp_cosine.shape[0])\n",
        "    ids[::2] += 1\n",
        "    ids[1::2] -= 1\n",
        "    exp_cosine_diag = tf.gather(exp_cosine, ids)\n",
        "    print (\"all pairs on diagonal\", exp_cosine_diag)\n",
        "    deno = tf.reduce_sum(exp_cosine_diag, axis=0) - tf.exp(1 / tau)\n",
        "    loss = tf.linalg.diag_part(exp_cosine_diag) / deno\n",
        "    log_loss = -tf.math.log(tf.reduce_mean(loss))\n",
        "    print (\"log loss\", log_loss)\n",
        "    return log_loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojyhtb4u9cS-"
      },
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "log_dir= \"/content/gdrive/My Drive/logs_he_regu/\"\n",
        "checkpoint_path = \"/content/gdrive/My Drive/models_he_regu/model_{epoch}.ckpt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES7oVl5xB_XF"
      },
      "source": [
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "      1e-5,\n",
        "      decay_steps=500 * 600,\n",
        "      decay_rate=1,\n",
        "      staircase=False)\n",
        "\n",
        "batch_size = 200\n",
        "\n",
        "# reload pretrained weights\n",
        "reload_checkpoint_path = '/content/gdrive/My Drive/models_he_regu/'\n",
        "latest = tf.train.latest_checkpoint(reload_checkpoint_path)\n",
        "print (latest)\n",
        "\n",
        "model = ResNet50(batch_size=batch_size)\n",
        "model.load_weights(latest)\n",
        "\n",
        "output = model.layers[-1].output\n",
        "\n",
        "# add loss function\n",
        "myloss = test_custom_cosine(output)\n",
        "model.add_loss(myloss)\n",
        "\n",
        "# compile model\n",
        "optimizer = tf.keras.optimizers.Adam(lr_schedule)\n",
        "model.compile(optimizer=optimizer)\n",
        "#tf.keras.utils.plot_model(model, \"/content/gdrive/My Drive/resnet_he.png\", show_shapes=True)\n",
        "#new_model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbHnFilLCWYA"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "print (\"### Found train samples {} test samples {}\".format(x_train.shape[0], x_test.shape[0]))\n",
        "\n",
        "train_gen = CIFAR10Sequence(x_train, batch_size=100, shuffle=True, aug=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-DJ8021Cb-M"
      },
      "source": [
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch=3)\n",
        "\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=checkpoint_path,\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "        monitor='loss',\n",
        "        mode='min',\n",
        "        verbose=1)]\n",
        "callbacks.append(tensorboard_callback)\n",
        "\n",
        "#training_history = model.fit(train_datagen.flow(X_train, Y_train, batch_size=128, shuffle=True), epochs=300, validation_data=valid_dataset, callbacks=callbacks)\n",
        "training_history = model.fit(train_gen, initial_epoch=1228, epochs=2000, callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40m3sN-HeATG"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def display_training_curves(training, title, subplot):\n",
        "  ax = plt.subplot(subplot)\n",
        "  ax.plot(training)\n",
        "  #ax.plot(validation)\n",
        "  ax.set_title('model '+ title)\n",
        "  ax.set_ylabel(title)\n",
        "  ax.set_xlabel('epoch')\n",
        "  ax.legend(['training'])\n",
        "\n",
        "plt.subplots(figsize=(10,10))\n",
        "plt.tight_layout()\n",
        "display_training_curves(training_history.history['loss'], 'loss', 211)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "155loTHVerbC"
      },
      "source": [
        "checkpoint_path = '/content/gdrive/My Drive/models/'\n",
        "latest = tf.train.latest_checkpoint(checkpoint_path)\n",
        "print (latest)\n",
        "\n",
        "new_model = ResNet50(batch_size=batch_size)\n",
        "new_model.load_weights(latest)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}